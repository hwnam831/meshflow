{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8' # Use 8 CPU devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.experimental.shard_map import shard_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = mesh_utils.create_device_mesh((4, 2))\n",
    "mesh = Mesh(devices, axis_names=('x', 'y'))\n",
    "\n",
    "a = jnp.arange( 8 * 16.).reshape(8, 16)\n",
    "b = jnp.arange(16 *  4.).reshape(16, 4)\n",
    "\n",
    "@partial(shard_map, mesh=mesh, in_specs=(P('x', 'y'), P('y', None)),\n",
    "         out_specs=P('x', None))\n",
    "def matmul_basic(a_block, b_block):\n",
    "  # a_block: f32[2, 8]\n",
    "  # b_block: f32[8, 4]\n",
    "  c_partialsum = jnp.dot(a_block, b_block)\n",
    "  c_block = jax.lax.psum(c_partialsum, 'y')\n",
    "  # c_block: f32[2, 4]\n",
    "  return c_block\n",
    "\n",
    "c = matmul_basic(a, b)   # c: f32[8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map, tree_all\n",
    "\n",
    "def allclose(a, b):\n",
    "  return tree_all(tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))\n",
    "\n",
    "allclose(c, jnp.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.debug.visualize_array_sharding(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import NamedSharding\n",
    "\n",
    "a = jax.device_put(a, NamedSharding(mesh, P('x', 'y')))\n",
    "b = jax.device_put(b, NamedSharding(mesh, P('y', None)))\n",
    "\n",
    "@jax.jit\n",
    "def matmul_reference(a, b):\n",
    "  c = jnp.dot(a, b)\n",
    "  return jax.lax.with_sharding_constraint(c, NamedSharding(mesh, P('x', None)))\n",
    "\n",
    "c_ref = matmul_reference(a, b)\n",
    "allclose(c_ref, jnp.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a blocks:'); jax.debug.visualize_array_sharding(a)\n",
    "print('b blocks:'); jax.debug.visualize_array_sharding(b)\n",
    "print('c blocks:'); jax.debug.visualize_array_sharding(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "devices = np.array(jax.devices()[:4])\n",
    "mesh = Mesh(devices, ('i',))  # mesh.shape['i'] = 4\n",
    "\n",
    "def check_shmap(f, y):\n",
    "  ans = shard_map(f, mesh, in_specs=P('i'), out_specs=P('i'))(y)\n",
    "  expected = jnp.concatenate([f(y_blk) for y_blk in jnp.split(y, mesh.shape['i'])])\n",
    "  print(allclose(ans, expected))\n",
    "\n",
    "\n",
    "check_shmap(lambda x: x.T @ x, jnp.arange(32).reshape(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,H,D = (4,16, 12,32)\n",
    "\n",
    "devices = mesh_utils.create_device_mesh((4, 2))\n",
    "mesh = Mesh(devices, axis_names=('row', 'col'))\n",
    "\n",
    "X = jnp.arange( B*S*H*D).reshape(B*S, H*D)\n",
    "W = jnp.arange(H*D*4*H*D).reshape(H*D, 4*H*D)\n",
    "\n",
    "Xt = jax.device_put(X, NamedSharding(mesh, P('row', 'col')))\n",
    "Wt = jax.device_put(W, NamedSharding(mesh, P('row', 'col')))\n",
    "\n",
    "@partial(shard_map, mesh=mesh, in_specs=(P('row', 'col'), P('row', 'col')),\n",
    "         out_specs=P('row', 'col'))\n",
    "def GSPMD_OS(Xij, Wij):\n",
    "    Xi = jax.lax.all_gather(Xij, 'col', tiled=True, axis=1)\n",
    "    print(Xi.shape)\n",
    "    Wj = jax.lax.all_gather(Wij, 'row', tiled=True, axis=0)\n",
    "    print(Wj.shape)\n",
    "    return Xi @ Wj\n",
    "\n",
    "y_ref = GSPMD_OS(X, W)\n",
    "#y_ref.shape\n",
    "#jnp.dot(X, W).shape\n",
    "allclose(y_ref, jnp.dot(X, W))\n",
    "y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = jnp.arange( B*S*4*H*D).reshape(B*S, 4*H*D)\n",
    "W1 = jnp.arange(H*D*4*H*D).reshape(4*H*D, H*D)\n",
    "\n",
    "Xt = jax.device_put(X1, NamedSharding(mesh, P('row', 'col')))\n",
    "Wt = jax.device_put(W1, NamedSharding(mesh, P('row', 'col')))\n",
    "\n",
    "@partial(shard_map, mesh=mesh, in_specs=(P('row', 'col'), P('col', 'row')),\n",
    "         out_specs=P('row', 'col'))\n",
    "def GSPMD_IS(Xij, Wij):\n",
    "    Wj = jax.lax.all_gather(Wij, 'row', tiled=True, axis=1)\n",
    "    print(Wj.shape)\n",
    "    Yp = Xij @ Wj\n",
    "    return jax.lax.psum_scatter(Yp, 'col', scatter_dimension=1, tiled=True)\n",
    "\n",
    "y_ref = GSPMD_IS(Xt, Wt)\n",
    "y_ref.shape\n",
    "#jnp.dot(X, W).shape\n",
    "allclose(y_ref, jnp.dot(X1, W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
