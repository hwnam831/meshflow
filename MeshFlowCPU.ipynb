{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MeshFlow Algorithms in Jax CPU\n",
    "\n",
    "This notebook shows correctness of MeshFlow and other 2D GeMM algorithms.\n",
    "\n",
    "Instead of using a real 2D device mesh, we use Jax's CPU emulation of multi-device mesh.\n",
    "\n",
    "You only require CPU version of Jax to run this notebook.\n",
    "\n",
    "To understand the code in detail, please check out Jax shard\\_map [tutorial](https://jax.readthedocs.io/en/latest/notebooks/shard_map.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#This allows emulating multi-device mesh with CPU threads.\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "#Please see TensorParallel.py for 2D GeMM implementations.\n",
    "from TensorParallel import SPMD, createMultihostMatrix\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.sharding import NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.tree_util import tree_map, tree_all\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "#The function to check the correctness of the output.\n",
    "def allclose(a, b):\n",
    "  return tree_all(tree_map(partial(jnp.allclose, atol=1e-2, rtol=1e-2), a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batsh size, input dimension, and output dimension\n",
    "B,I,O = (512, 256, 1024)\n",
    "\n",
    "#8 devices mapped to a (4x2) device mesh\n",
    "devices = mesh_utils.create_device_mesh((4, 2))\n",
    "mesh = Mesh(devices, axis_names=('row', 'col'))\n",
    "\n",
    "#Create the reference matrices\n",
    "X = jnp.arange(B*I,dtype=jnp.float32).reshape(B,I)/(B*I)\n",
    "W = jnp.arange(I*O,dtype=jnp.float32).reshape(I, O) / (I*O)\n",
    "Y = X@W\n",
    "\n",
    "#First, check the correctness of collective algorithm\n",
    "collective = SPMD(mesh,'collective')\n",
    "\n",
    "#Partition the matrices to the device mesh\n",
    "X_p = jax.device_put(X, NamedSharding(mesh, P('row', 'col')))\n",
    "W_p = jax.device_put(W, NamedSharding(mesh, P('row', 'col')))\n",
    "\n",
    "#Get the collective output-stationary algorithm\n",
    "collective = SPMD(mesh,'collective')\n",
    "collective_os = collective.OS()\n",
    "\n",
    "#Compare the collective OS result with the reference\n",
    "Y_p = collective_os(X_p, W_p)\n",
    "print(allclose(Y_p,Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we demonstrate the backpropagation computations using the dataflows.\n",
    "#The input for the backward pass is the output gradient, dY.\n",
    "\n",
    "dY = jnp.ones(B*O,dtype=jnp.float32).reshape(B,O)/(B*O)\n",
    "dY_p = jax.device_put(dY, NamedSharding(mesh, P('row', 'col')))\n",
    "\n",
    "#Backward data pass computes dX = dY * W^T\n",
    "dX = dY @ W.transpose()\n",
    "\n",
    "#This can be computed via LS algorithm, LS(dY, W) = dY * W^T\n",
    "collective_ls = collective.LS()\n",
    "dX_p = collective_ls(dY_p, W_p)\n",
    "\n",
    "print(allclose(dX_p, dX))\n",
    "\n",
    "#Backward weight pass computes dW = X^T * dY\n",
    "dW = X.transpose() @ dY\n",
    "\n",
    "#RS algorithm computes this: RS(X, dY) = X^T * dY\n",
    "collective_rs = collective.RS()\n",
    "dW_p = collective_rs(X_p, dY_p)\n",
    "\n",
    "print(allclose(dW_p, dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hereby we verify MeshFlow algorithm using collective algorithm as a reference.\n",
    "meshflow = SPMD(mesh, 'meshflow')\n",
    "meshflow_os = meshflow.OS()\n",
    "meshflow_ls = meshflow.LS()\n",
    "meshflow_rs = meshflow.RS()\n",
    "\n",
    "Y_p2 = meshflow_os(X_p, W_p)\n",
    "dX_p2 = meshflow_ls(dY_p, W_p)\n",
    "dW_p2 = meshflow_rs(X_p, dY_p)\n",
    "\n",
    "print(allclose(Y_p, Y_p2))\n",
    "print(allclose(dX_p, dX_p2))\n",
    "print(allclose(dW_p, dW_p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
